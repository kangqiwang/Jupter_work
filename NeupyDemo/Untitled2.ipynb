{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "F:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from neupy.algorithms import PNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Start classify iris dataset\n",
      "Test #1 : Guessed 15 out of 15\n",
      "Test #2 : Guessed 14 out of 15\n",
      "Test #3 : Guessed 15 out of 15\n",
      "Test #4 : Guessed 14 out of 15\n",
      "Test #5 : Guessed 13 out of 15\n",
      "Test #6 : Guessed 15 out of 15\n",
      "Test #7 : Guessed 13 out of 15\n",
      "Test #8 : Guessed 15 out of 15\n",
      "Test #9 : Guessed 15 out of 15\n",
      "Test #10: Guessed 15 out of 15\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_iris()\n",
    "data, target = dataset.data, dataset.target\n",
    "\n",
    "print(\"> Start classify iris dataset\")\n",
    "skfold = StratifiedKFold(n_splits=10)\n",
    "\n",
    "for i, (train, test) in enumerate(skfold.split(data, target), start=1):\n",
    "    x_train, x_test = data[train], data[test]\n",
    "    y_train, y_test = target[train], target[test]\n",
    "\n",
    "    pnn_network = PNN(std=0.1, verbose=False)\n",
    "    pnn_network.train(x_train, y_train)\n",
    "    result = pnn_network.predict(x_test)\n",
    "\n",
    "    n_predicted_correctly = np.sum(result == y_test)\n",
    "    n_test_samples = test.size\n",
    "\n",
    "    print(\"Test #{:<2}: Guessed {} out of {}\".format(\n",
    "        i, n_predicted_correctly, n_test_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1961  0.9806]\n",
      " [-0.1961  0.9806]\n",
      " [ 0.9806  0.1961]\n",
      " [ 0.9806 -0.1961]\n",
      " [-0.5812 -0.8137]\n",
      " [-0.8137 -0.5812]]\n",
      "\n",
      "\n",
      "[[ 0.1961 -0.1961  0.9806  0.9806 -0.5812 -0.8137]]\n",
      "\n",
      "\n",
      "[[ 0.1961 -0.1961  0.9806  0.9806 -0.5812 -0.8137]\n",
      " [ 0.9806  0.9806  0.1961 -0.1961 -0.8137 -0.5812]]\n",
      "\n",
      "\n",
      "[[ 0.1961  0.9806]\n",
      " [-0.1961  0.9806]\n",
      " [ 0.9806  0.1961]\n",
      " [ 0.9806 -0.1961]\n",
      " [-0.5812 -0.8137]\n",
      " [-0.8137 -0.5812]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_data = np.array([\n",
    "    [0.1961, 0.9806],\n",
    "    [-0.1961, 0.9806],\n",
    "    [0.9806, 0.1961],\n",
    "    [0.9806, -0.1961],\n",
    "    [-0.5812, -0.8137],\n",
    "    [-0.8137, -0.5812],\n",
    "])\n",
    "print(input_data)\n",
    "print('\\n')\n",
    "print(input_data.T[0:1, :])\n",
    "print('\\n')\n",
    "print(input_data.T[0:2,:])\n",
    "print('\\n')\n",
    "\n",
    "plt.plot(input_data.T[0:1, :], input_data.T[1:2, :], 'ko')\n",
    "print(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.35e-01 7.30e-06 6.70e-02 ... 0.00e+00 0.00e+00 0.00e+00]\n",
      " [1.43e-01 7.10e-06 7.30e-02 ... 0.00e+00 0.00e+00 0.00e+00]\n",
      " [1.62e-01 8.00e-06 8.70e-02 ... 0.00e+00 0.00e+00 0.00e+00]\n",
      " ...\n",
      " [4.34e-01 2.95e-05 1.78e-01 ... 0.00e+00 0.00e+00 0.00e+00]\n",
      " [4.64e-01 3.07e-05 1.95e-01 ... 0.00e+00 0.00e+00 0.00e+00]\n",
      " [4.74e-01 3.07e-05 1.79e-01 ... 0.00e+00 0.00e+00 0.00e+00]]\n",
      "this is my second data\n",
      "\n",
      "\n",
      "[[1.48800e+00 9.02130e-05 9.00000e-01 ... 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00]\n",
      " [7.28000e-01 3.76980e-05 3.53000e-01 ... 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00]\n",
      " [1.22000e+00 7.40410e-05 7.32000e-01 ... 1.00000e+00 1.78000e-01\n",
      "  1.00000e+00]\n",
      " ...\n",
      " [2.91200e+00 2.26205e-04 1.14100e+00 ... 1.00000e+00 4.57400e+00\n",
      "  0.00000e+00]\n",
      " [3.78800e+00 2.74883e-04 1.78600e+00 ... 3.00000e+00 1.95750e+01\n",
      "  0.00000e+00]\n",
      " [1.80900e+00 1.48510e-04 6.80000e-01 ... 1.00000e+00 3.38700e+00\n",
      "  0.00000e+00]]\n",
      "\n",
      "\n",
      "float64\n",
      "this is my finished data\n",
      "[[1.06200e+00 5.43390e-05 2.76000e-01 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [1.91700e+00 1.53518e-04 5.89000e-01 ... 1.00000e+00 4.10400e+00\n",
      "  0.00000e+00]\n",
      " [9.74000e-01 6.87920e-05 5.73000e-01 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " ...\n",
      " [1.71300e+00 8.65310e-05 7.29000e-01 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [5.34200e+00 3.10772e-04 2.40400e+00 ... 6.00000e+00 1.83370e+01\n",
      "  0.00000e+00]\n",
      " [2.94900e+00 2.90378e-04 1.50100e+00 ... 1.00000e+00 2.30930e+01\n",
      "  0.00000e+00]]\n",
      "\n",
      "\n",
      "[[2.16500e+00 1.22725e-04 1.08800e+00 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [1.81000e+00 1.71405e-04 8.65000e-01 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [4.49000e-01 2.10520e-05 2.08000e-01 ... 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00]\n",
      " ...\n",
      " [8.26000e-01 5.72710e-05 2.44000e-01 ... 1.00000e+00 3.34200e+00\n",
      "  0.00000e+00]\n",
      " [4.37900e+00 2.55401e-04 2.33300e+00 ... 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00]\n",
      " [3.14200e+00 1.84233e-04 1.53500e+00 ... 2.00000e+00 2.60900e+01\n",
      "  0.00000e+00]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0.\n",
      " 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.\n",
      " 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0.\n",
      " 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1.\n",
      " 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1.\n",
      " 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1.\n",
      " 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0.]\n",
      "\n",
      "\n",
      "27\n",
      "\n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "float64\n",
      "\n",
      "Main information\n",
      "\n",
      "[ALGORITHM] RMSProp\n",
      "\r\n",
      "[OPTION] batch_size = 64\n",
      "[OPTION] verbose = True\n",
      "[OPTION] epoch_end_signal = None\n",
      "[OPTION] show_epoch = 1\n",
      "[OPTION] shuffle_data = False\n",
      "[OPTION] step = 0.05\n",
      "[OPTION] train_end_signal = None\n",
      "[OPTION] error = mse\n",
      "[OPTION] addons = None\n",
      "[OPTION] decay = 0.95\n",
      "[OPTION] epsilon = 1e-05\n",
      "\r\n",
      "[THEANO] Initializing Theano variables and functions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time: 0:00:00 |N/A%|                           | ETA:  --:--:-- | error: ------"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[THEANO] Initialization finished successfully. It took 0.37 seconds\n",
      "\n",
      "Start training\n",
      "\n",
      "[TRAINING DATA] shapes: (832, 27)\n",
      "[TEST DATA] shapes: (208, 27)\n",
      "[TRAINING] Total epochs: 20\n",
      "\r\n",
      "---------------------------------------------------------\n",
      "|    Epoch    |  Train err  |  Valid err  |    Time     |\n",
      "---------------------------------------------------------\n",
      "|           1 |     0.50361 |     0.48558 |       87 ms |\n",
      "|           2 |     0.50361 |     0.48558 |       87 ms |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|           3 |     0.50361 |     0.48558 |       95 ms |\n",
      "|           4 |     0.50361 |     0.48558 |       91 ms |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|           5 |     0.50361 |     0.48558 |      126 ms |\n",
      "|           6 |     0.50361 |     0.48558 |      109 ms |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time: 0:00:00 |N/A%|                           | ETA:  --:--:-- | error: ------"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|           7 |     0.50361 |     0.48558 |      106 ms |\n",
      "|           8 |     0.50361 |     0.48558 |      100 ms |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time: 0:00:00 |N/A%|                           | ETA:  --:--:-- | error: ------"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|           9 |     0.50361 |     0.48558 |      106 ms |\n",
      "|          10 |     0.50361 |     0.48558 |      105 ms |\n",
      "|          11 |     0.50361 |     0.48558 |       94 ms |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|          12 |     0.50361 |     0.48558 |      109 ms |\n",
      "|          13 |     0.50361 |     0.48558 |      114 ms |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|          14 |     0.50361 |     0.48558 |      106 ms |\n",
      "|          15 |     0.50361 |     0.48558 |      109 ms |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time: 0:00:00 |N/A%|                           | ETA:  --:--:-- | error: ------"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|          16 |     0.50361 |     0.48558 |      106 ms |\n",
      "|          17 |     0.50361 |     0.48558 |      101 ms |\n",
      "|          18 |     0.50361 |     0.48558 |       86 ms |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time: 0:00:00 |N/A%|                                           | ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|          19 |     0.50361 |     0.48558 |      108 ms |\n",
      "|          20 |     0.50361 |     0.48558 |      106 ms |\n",
      "---------------------------------------------------------\n",
      "\r\n",
      "Test accuracy: 51.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from neupy import algorithms, layers, environment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "environment.reproducible()\n",
    "environment.speedup()\n",
    "\n",
    "test_data=np.loadtxt('test_data.txt',dtype='float',delimiter=',')\n",
    "train_data=np.loadtxt('train_data.txt',dtype='float64',delimiter=',')\n",
    "\n",
    "target=train_data[:,28]\n",
    "train_data=np.delete(train_data,27,1)\n",
    "\n",
    "train_data=np.delete(train_data,0,1)\n",
    "\n",
    "y_target=test_data[:,27]\n",
    "test_data=np.delete(test_data,27,1)\n",
    "test_data=np.delete(test_data,0,1)\n",
    "\n",
    "print(test_data)\n",
    "print('this is my second data')\n",
    "print('\\n')\n",
    "print(train_data)\n",
    "print('\\n')\n",
    "print(train_data.dtype)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    train_data, target, test_size=0.2)\n",
    "n_time_steps=x_train.shape[1]\n",
    "print('this is my finished data')\n",
    "print(x_train)\n",
    "print('\\n')\n",
    "print(x_test)\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print(y_train)\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print(y_test)\n",
    "print('\\n')\n",
    "print(n_time_steps)\n",
    "print('\\n')\n",
    "print(n_categories)\n",
    "print('\\n')\n",
    "print(x_train.dtype)\n",
    "\n",
    "\n",
    "network = algorithms.RMSProp(\n",
    "    [\n",
    "        layers.Input(n_time_steps),\n",
    "        # shape: (n_samples, n_time_steps)\n",
    "\n",
    "        layers.Sigmoid(1),\n",
    "        # shape: (n_samples, 1)\n",
    "    ],\n",
    "\n",
    "    step=0.05,\n",
    "    verbose=True,\n",
    "    batch_size=64,\n",
    "\n",
    ")\n",
    "network.train(x_train, y_train, x_test, y_test, epochs=20)\n",
    "\n",
    "y_predicted = network.predict(x_test).round()\n",
    "accuracy = (y_predicted.T == y_test).mean()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Test accuracy: {:.2%}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([4, 4, 4, 4, 2, 1, 1, 5, 5, 2]) array([2, 4, 4, 2, 5])\n",
      " array([2, 2, 3, 5]) ... array([1, 1, 4, 4, 2, 5]) array([4, 1, 2, 5])\n",
      " array([2, 4, 4, 2, 3, 4, 2, 3, 4, 4, 2, 3, 5])]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0. 0. 0. ... 5. 5. 2.]\n",
      " [0. 0. 0. ... 4. 2. 5.]\n",
      " [0. 0. 0. ... 2. 3. 5.]\n",
      " ...\n",
      " [0. 0. 0. ... 4. 2. 5.]\n",
      " [0. 0. 0. ... 1. 2. 5.]\n",
      " [0. 0. 0. ... 2. 3. 5.]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0. 0. 0. ... 1. 4. 1.]\n",
      " [0. 0. 0. ... 4. 2. 5.]\n",
      " [0. 0. 0. ... 1. 1. 3.]\n",
      " ...\n",
      " [0. 0. 0. ... 4. 3. 3.]\n",
      " [0. 0. 0. ... 5. 2. 2.]\n",
      " [0. 0. 0. ... 3. 1. 5.]]\n",
      "\n",
      "\n",
      "[[0. 0. 0. ... 3. 1. 3.]\n",
      " [0. 0. 0. ... 4. 2. 5.]\n",
      " [0. 0. 0. ... 2. 2. 5.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 1. 5.]\n",
      " [0. 0. 0. ... 4. 3. 5.]\n",
      " [0. 0. 0. ... 1. 1. 5.]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[0 1 0 ... 0 0 1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[0 1 1 ... 1 0 1]\n",
      "\n",
      "\n",
      "29\n",
      "\n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from neupy import algorithms, layers, environment\n",
    "from neupy.datasets import reber\n",
    "\n",
    "\n",
    "environment.reproducible()\n",
    "environment.speedup()\n",
    "\n",
    "\n",
    "def add_padding(data):\n",
    "    n_sampels = len(data)\n",
    "    max_seq_length = max(map(len, data))\n",
    "\n",
    "    data_matrix = np.zeros((n_sampels, max_seq_length))\n",
    "    for i, sample in enumerate(data):\n",
    "        data_matrix[i, -len(sample):] = sample\n",
    "\n",
    "    return data_matrix\n",
    "\n",
    "\n",
    "# An example of possible values for the `data` and `labels`\n",
    "# variables\n",
    "#\n",
    "# >>> data\n",
    "# array([array([1, 3, 1, 4]),\n",
    "#        array([0, 3, 0, 3, 0, 4, 3, 0, 4, 4]),\n",
    "#        array([0, 3, 0, 0, 3, 0, 4, 2, 4, 1, 0, 4, 0])], dtype=object)\n",
    "# >>>\n",
    "# >>> labels\n",
    "# array([1, 0, 0])\n",
    "data, labels = reber.make_reber_classification(\n",
    "    n_samples=10000, return_indeces=True)\n",
    "\n",
    "# Shift all indeces by 1. In the next row we will add zero\n",
    "# paddings, so we need to make sure that we will not confuse\n",
    "# paddings with zero indeces.\n",
    "data = data + 1\n",
    "\n",
    "# Add paddings at the beggining of each vector to make sure\n",
    "# that all samples has the same length. This trick allows to\n",
    "# train network with multiple independent samples.\n",
    "print(data)\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "\n",
    "data = add_padding(data)\n",
    "\n",
    "print(data)\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data, labels, test_size=0.2)\n",
    "\n",
    "n_categories = len(reber.avaliable_letters) + 1  # +1 for zero paddings\n",
    "n_time_steps = x_train.shape[1]\n",
    "\n",
    "print(x_train)\n",
    "print('\\n')\n",
    "print(x_test)\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print(y_train)\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print(y_test)\n",
    "print('\\n')\n",
    "print(n_time_steps)\n",
    "print('\\n')\n",
    "print(n_categories)\n",
    "print('\\n')\n",
    "print(x_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[1. 1. 1. 1. 1.]\n",
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[2. 2. 2. 2. 2.]\n",
      "\n",
      " 3\n",
      " 3\n",
      " 3\n",
      " 3\n",
      " 3\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.Tensor(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "\n",
    "\n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out = a)\n",
    "print(a)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "<AddBackward0 object at 0x0000018006DD40B8>\n",
      "Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      " Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "x = Variable(torch.ones(2, 2), requires_grad = True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1118.0179\n",
      "  775.4217\n",
      " -985.9751\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "x = Variable(x, requires_grad = True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "    1     1\n",
      "[torch.FloatTensor of size 100x2]\n",
      "\n",
      "/n\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Variable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dd03ca276513>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# The code below is deprecated in Pytorch 0.4. Now, autograd directly supports tensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'this is my x value    '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Variable' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "View more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "My Youtube Channel: https://www.youtube.com/user/MorvanZhou\n",
    "Dependencies:\n",
    "torch: 0.4\n",
    "matplotlib\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# torch.manual_seed(1)    # reproducible\n",
    "\n",
    "# make fake data\n",
    "\n",
    "n_data = torch.ones(100, 2)\n",
    "print(n_data)\n",
    "x0 = torch.normal(2*n_data, 1)      # class0 x data (tensor), shape=(100, 2)\n",
    "print(\"/n\")\n",
    "\n",
    "y0 = torch.zeros(100)               # class0 y data (tensor), shape=(100, 1)\n",
    "x1 = torch.normal(-2*n_data, 1)     # class1 x data (tensor), shape=(100, 2)\n",
    "y1 = torch.ones(100)                # class1 y data (tensor), shape=(100, 1)\n",
    "x = torch.cat((x0, x1), 0).type(torch.FloatTensor)  # shape (200, 2) FloatTensor = 32-bit floating\n",
    "y = torch.cat((y0, y1), ).type(torch.LongTensor)    # shape (200,) LongTensor = 64-bit integer\n",
    "\n",
    "\n",
    "# The code below is deprecated in Pytorch 0.4. Now, autograd directly supports tensors\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "print('this is my x value    ',x)\n",
    "\n",
    "plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=y.data.numpy(), s=100, lw=0, cmap='RdYlGn')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.out = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "net = Net(n_feature=2, n_hidden=10, n_output=2)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.02)\n",
    "loss_func = torch.nn.CrossEntropyLoss()  # the target label is NOT an one-hotted\n",
    "\n",
    "plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(100):\n",
    "    out = net(x)                 # input x and predict based on x\n",
    "    loss = loss_func(out, y)     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "\n",
    "    if t % 2 == 0:\n",
    "        # plot and show learning process\n",
    "        plt.cla()\n",
    "        prediction = torch.max(out, 1)[1]\n",
    "        pred_y = prediction.data.numpy().squeeze()\n",
    "        target_y = y.data.numpy()\n",
    "        plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=pred_y, s=100, lw=0, cmap='RdYlGn')\n",
    "        accuracy = float((pred_y == target_y).astype(int).sum()) / float(target_y.size)\n",
    "        plt.text(1.5, -4, 'Accuracy=%.2f' % accuracy, fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18006876e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#导入必要的模块\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#产生测试数据\n",
    "x = np.arange(1,10)\n",
    "y = x\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "#设置标题\n",
    "ax1.set_title('Scatter Plot')\n",
    "#设置X轴标签\n",
    "plt.xlabel('X')\n",
    "#设置Y轴标签\n",
    "plt.ylabel('Y')\n",
    "#画散点图\n",
    "ax1.scatter(x,y,c = 'r',marker = 'o')\n",
    "#设置图标\n",
    "plt.legend('x1')\n",
    "#显示所画的图\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-105-fd2e33802883>, line 48)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-105-fd2e33802883>\"\u001b[1;36m, line \u001b[1;32m48\u001b[0m\n\u001b[1;33m    print \"Trained model in {:.4f} seconds\".format(end - start)\u001b[0m\n\u001b[1;37m                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Tuning libraries\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#Import supervised learning model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#Import Graphing modules\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "#Initialize the models\n",
    "clf = GaussianNB()\n",
    "clf2 = svm.SVC()\n",
    "clf3 = SGDClassifier(loss = \"hinge\")\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate = 1.0, max_depth =1, random_state =0 )\n",
    "\n",
    "\n",
    "#Data Visualization Values\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "import pylab\n",
    "\n",
    "\n",
    "\n",
    "#Training and Testing Functions\n",
    "\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    ''' Fits a classifier to the training data. '''\n",
    "    \n",
    "    start = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    \n",
    "    print \"Trained model in {:.4f} seconds\".format(end - start)\n",
    "\n",
    "    \n",
    "def predict_labels(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on F1 score. '''\n",
    "    \n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time()\n",
    "    \n",
    "    print \"Made predictions in {:.4f} seconds.\".format(end - start)\n",
    "    return f1_score(target.values, y_pred, pos_label=1)\n",
    "\n",
    "\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    ''' Train and predict using a classifer based on F1 score. '''\n",
    "    \n",
    "    print \"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train))\n",
    "    \n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    \n",
    "    print \"F1 score for training set: {:.4f}.\".format(predict_labels(clf, X_train, y_train))\n",
    "    print \"F1 score for test set: {:.4f}.\".format(predict_labels(clf, X_test, y_test))\n",
    "\n",
    "\n",
    "# Tuning / Optimization Functions\n",
    "\n",
    "def performance_metric(y_true, y_predict):\n",
    "    error = f1_score(y_true, y_predict, pos_label=1)\n",
    "    return error\n",
    "\n",
    "def fit_model(X, y):\n",
    "  \n",
    "    classifier = svm.SVC()\n",
    "\n",
    "    parameters = {'kernel':['poly', 'rbf', 'sigmoid'], 'degree':[1, 2, 3], 'C':[0.1, 1, 10]}\n",
    "\n",
    "\n",
    "    f1_scorer = make_scorer(performance_metric,\n",
    "                                   greater_is_better=True)\n",
    "\n",
    "    clf = GridSearchCV(classifier,\n",
    "                       param_grid=parameters,\n",
    "                       scoring=f1_scorer)\n",
    "\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "# Read student data\n",
    "parkinson_data = pd.read_csv(\"parkinsons.csv\")\n",
    "print \"Student data read successfully!\"\n",
    "\n",
    "#Data Exploration\n",
    "\n",
    "#Number of patients\n",
    "n_patients = parkinson_data.shape[0]\n",
    "\n",
    "#Number of features\n",
    "n_features = parkinson_data.shape[1]-1\n",
    "\n",
    "#With Parkinsons\n",
    "n_parkinsons = parkinson_data[parkinson_data['status'] == 1].shape[0]\n",
    "\n",
    "#Without Parkinsons\n",
    "n_healthy = parkinson_data[parkinson_data['status'] == 0].shape[0]\n",
    "\n",
    "#Result Output\n",
    "print \"Total number of patients: {}\".format(n_patients)\n",
    "print \"Number of features: {}\".format(n_features)\n",
    "print \"Number of patients with Parkinsons: {}\".format(n_parkinsons)\n",
    "print \"Number of patients without Parkinsons: {}\".format(n_healthy)\n",
    "\n",
    "#Preparing the Data\n",
    "\n",
    "# Extract feature columns\n",
    "feature_cols = list(parkinson_data.columns[1:16]) + list(parkinson_data.columns[18:])\n",
    "target_col = parkinson_data.columns[17]\n",
    "\n",
    "# Show the list of columns\n",
    "print \"Feature columns:\\n{}\".format(feature_cols)\n",
    "print \"\\nTarget column: {}\".format(target_col)\n",
    "\n",
    "# Separate the data into feature data and target data (X_all and y_all, respectively)\n",
    "X_all = parkinson_data[feature_cols]\n",
    "y_all = parkinson_data[target_col]\n",
    "\n",
    "# Show the feature information by printing the first five rows\n",
    "print \"\\nFeature values:\"\n",
    "print X_all.head()\n",
    "\n",
    "# Training and Testing Data Split\n",
    "num_all = parkinson_data.shape[0] \n",
    "num_train = 150 # about 75% of the data\n",
    "num_test = num_all - num_train\n",
    "\n",
    "# Select features and corresponding labels for training/test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=num_test,random_state=5)\n",
    "print \"Shuffling of data into test and training sets complete!\"\n",
    "\n",
    "print \"Training set: {} samples\".format(X_train.shape[0])\n",
    "print \"Test set: {} samples\".format(X_test.shape[0])\n",
    "\n",
    "X_train_50 = X_train[:50]\n",
    "y_train_50 = y_train[:50]\n",
    "\n",
    "X_train_100 = X_train[:100]\n",
    "y_train_100 = y_train[:100]\n",
    "\n",
    "X_train_150 = X_train[:150]\n",
    "y_train_150 = y_train[:150]\n",
    "\n",
    "#Training the data\n",
    "\n",
    "#50 set\n",
    "print \"Naive Bayes:\"\n",
    "train_predict(clf,X_train_50,y_train_50,X_test,y_test)\n",
    "\n",
    "print \"Support Vector Machines:\"\n",
    "train_predict(clf2,X_train_50,y_train_50,X_test,y_test)\n",
    "\n",
    "print \"Stochastic Gradient Descent:\"\n",
    "train_predict(clf3,X_train_50,y_train_50,X_test,y_test)\n",
    "\n",
    "print \"Gradient Tree Boosting:\"\n",
    "train_predict(clf4,X_train_50,y_train_50,X_test,y_test)\n",
    "\n",
    "#100 set\n",
    "\n",
    "print \"Naive Bayes:\"\n",
    "train_predict(clf,X_train_100,y_train_100,X_test,y_test)\n",
    "\n",
    "print \"Support Vector Machines:\"\n",
    "train_predict(clf2,X_train_100,y_train_100,X_test,y_test)\n",
    "\n",
    "print \"Stochastic Gradient Descent:\"\n",
    "train_predict(clf3,X_train_100,y_train_100,X_test,y_test)\n",
    "\n",
    "print \"Gradient Tree Boosting:\"\n",
    "train_predict(clf4,X_train_100,y_train_100,X_test,y_test)\n",
    "\n",
    "#150 set\n",
    "\n",
    "print \"Naive Bayes:\"\n",
    "train_predict(clf,X_train_150,y_train_150,X_test,y_test)\n",
    "\n",
    "print \"Support Vector Machines:\"\n",
    "train_predict(clf2,X_train_150,y_train_150,X_test,y_test)\n",
    "\n",
    "print \"Stochastic Gradient Descent:\"\n",
    "train_predict(clf3,X_train_150,y_train_150,X_test,y_test)\n",
    "\n",
    "print \"Gradient Tree Boosting:\"\n",
    "train_predict(clf4,X_train_150,y_train_150,X_test,y_test)\n",
    "\n",
    "###################\n",
    "\n",
    "#Data Visualization\n",
    "\n",
    "#This produces the scatter matrix from my data set. I have commented it out for now.\n",
    "\n",
    "# pd.scatter_matrix(parkinson_data, alpha = 0.3, figsize = (30,30), diagonal = 'kde');\n",
    "# pylab.savefig(\"scatter\" + \".png\")\n",
    "\n",
    "###################\n",
    "\n",
    "#I got the supervised model to be trained from my data set\n",
    "#Now to tune it to get the optimal model for prediction\n",
    "\n",
    "#Tuning model (Support Vector Machine)\n",
    "\n",
    "print \"Tuning the model. This may take a while.....\"\n",
    "\n",
    "clf2 = fit_model(X_train, y_train)\n",
    "print \"Successfully fit a model!\"\n",
    "\n",
    "print \"The best parameters were: \" \n",
    "\n",
    "print clf2.best_params_\n",
    "\n",
    "start = time()\n",
    "    \n",
    "print \"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf2, X_train, y_train))\n",
    "print \"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf2, X_test, y_test))\n",
    "\n",
    "end = time()\n",
    "    \n",
    "print \"Tuned model in {:.4f} seconds.\".format(end - start)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
